# Document Intelligence üß†

**Document Intelligence** is a high-performance Java utility built on the **LangChain4j** framework. It leverages local LLM embeddings via **Ollama** to rank and retrieve the most relevant sections of text from your local directories using semantic search.

By utilizing **Java 21 Virtual Threads (Project Loom)**, the system processes large document sets in parallel, ensuring high-speed indexing and retrieval.

---

## üöÄ Key Features

* **100% Local & Private:** Powered by `nomic-embed-text` via Ollama. No data is sent to the cloud.
* **Parallel Processing:** Uses a Virtual Thread executor to handle file extraction and chunking concurrently.
* **Context-Aware Splitting:** Uses a **Recursive Character Splitter** with overlapping chunks to prevent losing information at the edges of a "cut."
* **Semantic Ranking:** Uses **Cosine Similarity** to find answers based on meaning, not just exact keyword matches.
* **Smart Metadata:** Automatically tracks the source file for every text segment retrieved.

---

## üìÇ Supported File Types

The engine automatically filters and processes the following formats:

| Format | Extension | Description |
| :--- | :--- | :--- |
| **Documents** | `.pdf`, `.docx` | Fixed layout and rich text word documents. |
| **Technical** | `.md`, `.markdown` | Documentation and developer notes. |
| **Data** | `.csv`, `.json` | Structured data and configuration logs. |
| **Plain Text** | `.txt` | Standard raw text files. |

---

## üõ†Ô∏è Prerequisites

1.  **Java 21 or higher** (Required for Virtual Thread support).
2.  **Ollama** installed and running on `localhost:11434`.
3.  **Embedding Model:** Pull the default model by running:
    ```bash
    ollama pull nomic-embed-text
    ```

---

## ‚öôÔ∏è Core Logic Configuration

The system uses the following defaults for optimal performance with Local LLMs:

* **Chunk Size:** `500` characters (ensures segments are focused and digestible).
* **Chunk Overlap:** `100` characters (provides "connective tissue" between segments).
* **Embedding Model:** `nomic-embed-text:latest`.
* **Top-K Results:** Returns the **5** most relevant matches per query.

---

## üìñ How It Works

1.  **Ingestion:** The app scans your provided directory and filters for supported files.
2.  **Chunking:** The `DocumentSplitter` breaks long texts into 500-character pieces.
3.  **Vectorization:** Ollama converts your text segments into high-dimensional vectors (embeddings).
4.  **Querying:** When you ask a question, your query is also converted into a vector.
5.  **Similarity Match:** The system calculates the "distance" between your query vector and all document vectors, returning the closest matches.

---

## üñ•Ô∏è Usage

1.  **Start Ollama** on your machine.
2.  **Run the application:** `java com.intelligence.TextSimilarityRanker.java`
3.  **Provide Directory:** Input the path to your documents when prompted.
4.  **Query:** Type your question.
    * *Example:* "What are the project requirements for the 2026 launch?"
5.  **Analyze:** View the ranked results, including similarity scores (0.0 to 1.0) and source file names.

---

*Generated by Document Intelligence v1.0*